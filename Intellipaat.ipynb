{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc575c73",
   "metadata": {},
   "source": [
    "## Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ef9eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Practice-1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "462cf7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7cf86e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fad48",
   "metadata": {},
   "source": [
    "## Creating Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "567437d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf=SparkConf()\n",
    "conf.setMaster('local')\n",
    "conf.setAppName(\"Practice-2\")\n",
    "sc=SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "057f3559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.8'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "571442a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db749796",
   "metadata": {},
   "source": [
    "# PySpark Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602e14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod(x):\n",
    "    return (x,x%2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4bbe276",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.parallelize(range(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "322afee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2404663910344\n"
     ]
    }
   ],
   "source": [
    "old_rdd=(id(rdd))\n",
    "print(old_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e7b151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=rdd.map(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81bf859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2404286265480\n"
     ]
    }
   ],
   "source": [
    "new_rdd=(id(rdd))\n",
    "print(new_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63b57bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "import ctypes\n",
    "print (ctypes.cast(old_rdd, ctypes.py_object).value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3860eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[2] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "import ctypes\n",
    "print (ctypes.cast(new_rdd, ctypes.py_object).value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39d1ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del rdd\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f1ab23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 0), (3, 1), (4, 0), (5, 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab7e7d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 0), (3, 1), (4, 0), (5, 1), (6, 0), (7, 1), (8, 0), (9, 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cbc533b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 1), (2, 0), (3, 1), (4, 0), (5, 1), (6, 0), (7, 1), (8, 0), (9, 1)]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0bb1015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f52c25e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6], [7, 8, 9]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize(range(1,10),3)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "669f2d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 7, 8, 9], [4, 5, 6]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=rdd.repartition(2)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "225daabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [1, 2, 3], [7, 8, 9], [4, 5, 6], []]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=rdd.repartition(5)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23539417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 7, 8, 9, 4, 5, 6]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=rdd.repartition(1)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c579b",
   "metadata": {},
   "source": [
    "## Read from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "dbca4c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Transformations in Spark are “lazy”, meaning that they do not compute their results right away. Instead, they just “remember” the operation to be performed and the dataset (e.g., file) to which the operation is to be performed. The transformations are only actually computed when an action is called and the result is returned to the driver program. This design enables Spark to run more efficiently. For example, if a big file was transformed in various ways and passed to first action, Spark would only process and return the result for the first line, rather than do the work for the entire file.'],\n",
       " ['',\n",
       "  'By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist or cache method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it.']]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrdd=sc.textFile(\"textfiles/Spark.txt\")\n",
    "# textrdd.take(2)\n",
    "textrdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ce52b",
   "metadata": {},
   "source": [
    "## word count (method-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "4854ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(lines):\n",
    "    lines=lines.lower()\n",
    "    lines=lines.split(' ')\n",
    "    return lines\n",
    "flatrdd=textrdd.flatMap(f)\n",
    "# flatrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "386c6d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  'TransformationsinSparkare“lazy”,meaningthattheydonotcomputetheirresultsrightaway.Instead,theyjust“remember”theoperationtobeperformedandthedataset(e.g.,file)towhichtheoperationistobeperformed.Thetransformationsareonlyactuallycomputedwhenanactioniscalledandtheresultisreturnedtothedriverprogram.ThisdesignenablesSparktorunmoreefficiently.Forexample,ifabigfilewastransformedinvariouswaysandpassedtofirstaction,Sparkwouldonlyprocessandreturntheresultforthefirstline,ratherthandotheworkfortheentirefile.Bydefault,eachtransformedRDDmayberecomputedeachtimeyourunanactiononit.However,youmayalsopersistanRDDinmemoryusingthepersistorcachemethod,inwhichcaseSparkwillkeeptheelementsaroundontheclusterformuchfasteraccessthenexttimeyouqueryit.')]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rough demo\n",
    "def fun(x):\n",
    "    l=\"\"\n",
    "    for i in x:\n",
    "        l=l+i\n",
    "    return l\n",
    "words.map(lambda x:(1,x)).groupByKey().mapValues(fun).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0d9cfe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount=words.map(lambda x:(x,1))\n",
    "# wordcount.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "99a34b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount=wordcount.groupByKey()\n",
    "# wordcount.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "4d036852",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount=wordcount.mapValues(sum)\n",
    "# wordcount.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "9d33a780",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 317.0 failed 1 times, most recent failure: Lost task 0.0 in stage 317.0 (TID 1814, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 377, in main\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 372, in process\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 352, in func\n    return f(iterator)\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1055, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1055, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: <lambda>() missing 1 required positional argument: 'y'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2132)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor368.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 377, in main\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 372, in process\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 352, in func\n    return f(iterator)\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1055, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1055, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: <lambda>() missing 1 required positional argument: 'y'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DMS240~1\\AppData\\Local\\Temp/ipykernel_10552/1269441067.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwordcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwordcount\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwordcount\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msortByKey\u001b[1;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[1;31m# the key-space into bins such that the bins have roughly the same\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m# number of (key, value) pairs falling into them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m         \u001b[0mrddSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrddSize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m  \u001b[1;31m# empty RDD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m         \"\"\"\n\u001b[1;32m-> 1055\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1044\u001b[0m         \u001b[1;36m6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m         \"\"\"\n\u001b[1;32m-> 1046\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[1;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m         \u001b[1;31m# to the final reduce call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    814\u001b[0m         \"\"\"\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 816\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    817\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 317.0 failed 1 times, most recent failure: Lost task 0.0 in stage 317.0 (TID 1814, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 377, in main\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 372, in process\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 352, in func\n    return f(iterator)\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1055, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1055, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: <lambda>() missing 1 required positional argument: 'y'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2132)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor368.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 377, in main\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 372, in process\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 352, in func\n    return f(iterator)\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1055, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1055, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: <lambda>() missing 1 required positional argument: 'y'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2107)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "wordcount=wordcount.map(lambda x:(x[1],x[0])).sortByKey(False)\n",
    "wordcount.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ec7c2",
   "metadata": {},
   "source": [
    "## word count (method-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "78d7cfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transformations in Spark are “lazy”, meaning that they do not compute their results right away. Instead, they just “remember” the operation to be performed and the dataset (e.g., file) to which the operation is to be performed. The transformations are only actually computed when an action is called and the result is returned to the driver program. This design enables Spark to run more efficiently. For example, if a big file was transformed in various ways and passed to first action, Spark would only process and return the result for the first line, rather than do the work for the entire file.',\n",
       " 'By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist or cache method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it.']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonempty_lines=textrdd.filter(lambda x: len(x)>0)\n",
    "nonempty_lines.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e47b913c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transformations', 'in', 'Spark', 'are', '“lazy”,']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=nonempty_lines.flatMap(lambda x: x.split(' '))\n",
    "words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "dad7810d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  'TransformationsinSparkare“lazy”,meaningthattheydonotcomputetheirresultsrightaway.Instead,theyjust“remember”theoperationtobeperformedandthedataset(e.g.,file)towhichtheoperationistobeperformed.Thetransformationsareonlyactuallycomputedwhenanactioniscalledandtheresultisreturnedtothedriverprogram.ThisdesignenablesSparktorunmoreefficiently.Forexample,ifabigfilewastransformedinvariouswaysandpassedtofirstaction,Sparkwouldonlyprocessandreturntheresultforthefirstline,ratherthandotheworkfortheentirefile.Bydefault,eachtransformedRDDmayberecomputedeachtimeyourunanactiononit.However,youmayalsopersistanRDDinmemoryusingthepersistorcachemethod,inwhichcaseSparkwillkeeptheelementsaroundontheclusterformuchfasteraccessthenexttimeyouqueryit.')]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rough demo\n",
    "words.map(lambda x:(1,x)).reduceByKey(add).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "616a17e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Transformations', 1), ('in', 1), ('Spark', 1), ('are', 1), ('“lazy”,', 1)]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcount=words.map(lambda x:(x,1))\n",
    "wordcount.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "6af267bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('in', 4), ('Spark', 4), ('are', 2), ('meaning', 1), ('do', 2)]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcount=wordcount.reduceByKey(add)\n",
    "wordcount.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d513cc73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, 'the'), (6, 'to'), (4, 'in'), (4, 'Spark'), (4, 'and')]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcount=wordcount.map(lambda x:(x[1],x[0])).sortByKey(False)\n",
    "wordcount.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447c58ab",
   "metadata": {},
   "source": [
    "## caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7a82f3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textfiles/Spark.txt MapPartitionsRDD[121] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa574ee",
   "metadata": {},
   "source": [
    "## wholeTextFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5f794f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "textrdd=sc.wholeTextFiles(\"textfiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c29034c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/D:/Projects/Python/pyspark/textfiles/data.txt',\n",
       "  \"'The online encyclopedia project, Wikipedia, is the most popular wiki-based website, and is one of the most widely viewed sites in the world, having been ranked in the top twenty since 2007.[3] \"),\n",
       " ('file:/D:/Projects/Python/pyspark/textfiles/Spark.txt',\n",
       "  'A wiki (/ˈwɪki/ (About this soundlisten) WIK-ee) is a hypertext publication collaboratively edited and managed by its own audience directly using a web browser. A typical wiki contains multiple pages for the subjects or scope of the project and could be either open to the public or limited to use within an organization for maintaining its internal knowledge base.\\r\\n\\r\\nWikis are enabled by wiki software, otherwise known as wiki engines. A wiki engine, being a form of a content management system, differs from other web-based systems such as blog software, in that the content is created without any defined owner or leader, and wikis have little inherent structure, allowing structure to emerge according to the needs of the users.[1] Wiki engines usually allow content to be written using a simplified markup language and sometimes edited with the help of a rich-text editor.[2] There are dozens of different wiki engines in use, both standalone and part of other software, such as bug tracking systems. Some wiki engines are open source, whereas others are proprietary. Some permit control over different functions (levels of access); for example, editing rights may permit changing, adding, or removing material. Others may permit access without enforcing access control. Other rules may be imposed to organize content.\\r\\n\\r\\nWikipedia is not a single wiki but rather a collection of hundreds of wikis, with each one pertaining to a specific language. In addition to Wikipedia, there are hundreds of thousands of other wikis in use, both public and private, including wikis functioning as knowledge management resources, notetaking tools, community websites, and intranets. The English-language Wikipedia has the largest collection of articles: as of February 2020, it has over 6 million articles. Ward Cunningham, the developer of the first wiki software, WikiWikiWeb, originally described wiki as \"the simplest online database that could possibly work.\"[4] \"Wiki\" (pronounced [wiki][note 1]) is a Hawaiian word meaning \"quick.')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e73efc",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f77b8",
   "metadata": {},
   "source": [
    "## map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "33573b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', 5), ('rdd', 3), ('example', 7), ('sample', 6)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=sc.parallelize([\"spark\", \"rdd\", \"example\", \"sample\"])\n",
    "y=x.map(lambda x:(x,len(x)))\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7653d798",
   "metadata": {},
   "source": [
    "## Tuple map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "660c5d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('b', 2), ('c', 2)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('a' ,7),('b' ,2),('c' ,2)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "54cd8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changes(tup):\n",
    "    return (tup[0],len(tup[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6f1e8fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 1), ('c', 1)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=rdd.map(changes)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9761284d",
   "metadata": {},
   "source": [
    "## flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69f99769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 1, 2, 3]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=sc.parallelize([2,3,4])\n",
    "y=x.flatMap(lambda x: range(1,x))\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daed269",
   "metadata": {},
   "source": [
    "## filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3290efa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=sc.parallelize([1,2,3,4,5])\n",
    "x.filter(lambda x: x%2==0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97fb676",
   "metadata": {},
   "source": [
    "## sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e690a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 2, 4, 4, 5, 5, 9]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel=sc.parallelize(range(10))\n",
    "parallel=parallel.sample(True,1)\n",
    "parallel.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71e78e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 5, 5, 8, 9]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel=sc.parallelize(range(10))\n",
    "parallel=parallel.sample(True,0.5)\n",
    "parallel.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d982a5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel=sc.parallelize(range(10))\n",
    "parallel=parallel.sample(True,0.2)\n",
    "parallel.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cdc02f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel=sc.parallelize(range(10))\n",
    "parallel=parallel.sample(False,1)\n",
    "parallel.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af8b2419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 6, 7, 9]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel=sc.parallelize(range(10))\n",
    "parallel=parallel.sample(False,0.5)\n",
    "parallel.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38a2e398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 6]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel=sc.parallelize(range(10))\n",
    "parallel=parallel.sample(False,0.2)\n",
    "parallel.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62654d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel=sc.parallelize(range(10))\n",
    "parallel=parallel.sample(False,0.2)\n",
    "parallel.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686c933",
   "metadata": {},
   "source": [
    "## union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ede8463b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel=sc.parallelize(range(1,9))\n",
    "par=sc.parallelize(range(5,15))\n",
    "parallel.union(par).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d7bdb",
   "metadata": {},
   "source": [
    "## Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7276c58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel=sc.parallelize(range(1,9))\n",
    "par=sc.parallelize(range(5,15))\n",
    "parallel.intersection(par).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28daa4a",
   "metadata": {},
   "source": [
    "## distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad4e1542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel=sc.parallelize(range(1,9))\n",
    "par=sc.parallelize(range(5,15))\n",
    "parallel.union(par).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a510e",
   "metadata": {},
   "source": [
    "## sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8c96222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3, 5, 7]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=sc.parallelize([5,7,1,3,2,1])\n",
    "y.sortBy(lambda c: c,True).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b31af66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Z', 1), ('L', 5), ('H', 10), ('A', 26), ('A', 5)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=sc.parallelize([('H',10), ('A',26), ('Z',1), ('L',5), ('A',5)])\n",
    "y.sortBy(lambda c: c,False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc22d6c4",
   "metadata": {},
   "source": [
    "## mapPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "940f2ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize([1,2,3,4],2)\n",
    "def f(itr): yield sum(itr)\n",
    "rdd.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4eeea5",
   "metadata": {},
   "source": [
    "## mapPartitionsWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a382c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3), (1, 7), (2, 11), (3, 15)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize([1,2,3,4,5,6,7,8],4)\n",
    "def f(splitIndex,itr): yield (splitIndex,sum(itr))\n",
    "rdd.mapPartitionsWithIndex(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d7aadd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4], [5, 6], [7, 8]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693f338d",
   "metadata": {},
   "source": [
    "## groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c9258fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, <pyspark.resultiterable.ResultIterable at 0x22fe4d9f6c8>),\n",
       " (1, <pyspark.resultiterable.ResultIterable at 0x22fe4d9f708>)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize([1,2,3,4,5,6,7,8])\n",
    "result=rdd.groupBy(lambda x:x%2).collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0dea7423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 4, 6, 8]), (1, [1, 3, 5, 7])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x,sorted(y)) for (x,y) in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f887cc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (4, 2), (9, 3), (16, 4)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=sc.parallelize(range(0,5)).keyBy(lambda x:x*x)\n",
    "x.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70611743",
   "metadata": {},
   "source": [
    "## zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6611c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=sc.parallelize(range(0,5))\n",
    "y=sc.parallelize(range(1000,1005))\n",
    "x.zip(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d094506",
   "metadata": {},
   "source": [
    "## zip + groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bceabb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [[0], [0]]),\n",
       " (1, [[1], [1]]),\n",
       " (2, [[], [2]]),\n",
       " (3, [[], [3]]),\n",
       " (4, [[2], [4]]),\n",
       " (9, [[3], []]),\n",
       " (16, [[4], []])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=sc.parallelize(range(0,5)).keyBy(lambda x:x*x)\n",
    "y=sc.parallelize(zip(range(0,5), range(0,5)))\n",
    "[(a, list(map(list,b))) for a,b in sorted(x.cogroup(y).collect())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08445b34",
   "metadata": {},
   "source": [
    "## Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "39e7c754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1], [2, 3], [4, 5], [6, 7, 8]]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize(range(0,9),4)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "30f62bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 4, 5, 6, 7, 8], [2, 3]]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=rdd.repartition(2)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74938b76",
   "metadata": {},
   "source": [
    "## coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b13c7932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3], [4, 5]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize(range(1,6),3)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4aff306f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3, 4, 5]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=rdd.coalesce(2)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5359bdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3, 4, 5]]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=rdd.coalesce(3)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66237104",
   "metadata": {},
   "source": [
    "# Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615092b2",
   "metadata": {},
   "source": [
    "## reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ddf70153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add(x,y):\n",
    "    return x+y\n",
    "sc.parallelize(range(6)).reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623258c1",
   "metadata": {},
   "source": [
    "## first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "69aeb9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(6)).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4c3fd",
   "metadata": {},
   "source": [
    "## takeOrdered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6b8ac263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,5,3,9,4,0,2]).takeOrdered(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ed0c92",
   "metadata": {},
   "source": [
    "## take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dc10e163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 3, 9]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,5,3,9,4,0,2]).take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a96de3",
   "metadata": {},
   "source": [
    "## count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8c293d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,5,3,9,4,0,2]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe3b9ea",
   "metadata": {},
   "source": [
    "## collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2e059709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 2, 1, 4, 0, 2]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,5,2,1,4,0,2]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6279cefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 4, 5]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,5,2,1,4,0,2]).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "653634b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a', 2: 'b', 3: 'c'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha=sc.parallelize([(1,'a'), (2,'b'), (3,'c')])\n",
    "alpha.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc816b88",
   "metadata": {},
   "source": [
    "## saveAsTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ac0b0e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=sc.parallelize(range(1,10000),3)\n",
    "a.saveAsTextFile(\"output/sample1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f140c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=sc.parallelize(range(1,10),1)\n",
    "a.saveAsTextFile(\"output/sample2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5aa8d",
   "metadata": {},
   "source": [
    "## foreach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7093b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93b5309",
   "metadata": {},
   "source": [
    "## foreachPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "be545949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(itr):\n",
    "    for x in itr:\n",
    "        return (x)\n",
    "sc.parallelize([1,2,3,4,5]).foreachPartition(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c50cad9",
   "metadata": {},
   "source": [
    "## Mathematical Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d75c9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers=sc.parallelize(range(1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a8e36b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "56f9e8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f6a98799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b362a605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "816.6666666666666"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1e802504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4f9d6b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.577380332470412"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers.stdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c2289e",
   "metadata": {},
   "source": [
    "## countByValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3d9bdd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 4, 2: 3, 3: 4, 4: 2, 5: 1, 6: 1, 7: 1, 8: 1})"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=sc.parallelize([1,2,3,4,5,6,7,8,2,4,2,3,3,3,1,1,1])\n",
    "a.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8ce22",
   "metadata": {},
   "source": [
    "## toDebugString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "99021073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(6) PythonRDD[424] at RDD at PythonRDD.scala:53 []\\n |  MapPartitionsRDD[423] at mapPartitions at PythonRDD.scala:133 []\\n |  ShuffledRDD[422] at partitionBy at NativeMethodAccessorImpl.java:0 []\\n +-(6) PairwiseRDD[421] at subtract at C:\\\\Users\\\\DMS240~1\\\\AppData\\\\Local\\\\Temp/ipykernel_10552/2648738274.py:3 []\\n    |  PythonRDD[420] at subtract at C:\\\\Users\\\\DMS240~1\\\\AppData\\\\Local\\\\Temp/ipykernel_10552/2648738274.py:3 []\\n    |  UnionRDD[419] at union at NativeMethodAccessorImpl.java:0 []\\n    |  PythonRDD[417] at RDD at PythonRDD.scala:53 []\\n    |  ParallelCollectionRDD[415] at parallelize at PythonRDD.scala:195 []\\n    |  PythonRDD[418] at RDD at PythonRDD.scala:53 []\\n    |  ParallelCollectionRDD[416] at parallelize at PythonRDD.scala:195 []'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=sc.parallelize(range(1,19),3)\n",
    "b=sc.parallelize(range(1,13),3)\n",
    "c=a.subtract(b)\n",
    "c.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "45c85312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 13, 14, 15, 16, 17]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e337f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
