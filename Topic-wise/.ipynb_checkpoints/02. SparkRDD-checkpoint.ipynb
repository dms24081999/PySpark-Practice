{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b36c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa64da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: spark.stop()\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb30962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# By default 12 executors if not specified\n",
    "spark=SparkSession.builder.appName(\"RDD\").master(\"local[4]\").getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47206b",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406240af",
   "metadata": {},
   "source": [
    "### Parallelized Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ea0389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2), ('b', 2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize([(\"a\",7), (\"a\",2), (\"b\",2)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e39f24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('d', 1), ('b', 1), ('b', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2=sc.parallelize([('a',2), ('d',1), ('b',1), ('b',1)])\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51a80d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3=sc.parallelize(range(100))\n",
    "rdd3.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d551a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', ['x', 'y', 'z']), ('b', ['p', 'r'])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4=sc.parallelize([('a',['x','y','z']),\n",
    "                     ('b',['p','r'])])\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd7b89",
   "metadata": {},
   "source": [
    "### External data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "671dd1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Utilitatis causa amicitia est quaesita.',\n",
       " 'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Collatio igitur ista te nihil iuvat. Honesta oratio, Socratica, Platonis etiam. Primum in nostrane potestate est, quid meminerimus? Duo Reges: constructio interrete. Quid, si etiam iucunda memoria est praeteritorum malorum? Si quidem, inquit, tollerem, sed relinquo. An nisi populari fama?',\n",
       " '',\n",
       " 'Quamquam id quidem licebit iis existimare, qui legerint. Summum a vobis bonum voluptas dicitur. At hoc in eo M. Refert tamen, quo modo. Quid sequatur, quid repugnet, vident. Iam id ipsum absurdum, maximum malum neglegi.',\n",
       " 'Aeque enim contingit omnibus fidibus, ut incontentae sint.',\n",
       " 'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quae cum ita sint, effectum est nihil esse malum, quod turpe non sit. Itaque nostrum est-quod nostrum dico, artis est-ad ea principia, quae accepimus. Quod totum contra est. Duo Reges: constructio interrete. Atqui iste locus est, Piso, tibi etiam atque etiam confirmandus, inquam; Quamvis enim depravatae non sint, pravae tamen esse possunt. Duarum enim vitarum nobis erunt instituta capienda.',\n",
       " '',\n",
       " 'Non igitur de improbo, sed de callido improbo quaerimus, qualis Q. Audio equidem philosophi vocem, Epicure, sed quid tibi dicendum sit oblitus es. Ex ea difficultate illae fallaciloquae, ut ait Accius, malitiae natae sunt. At multis malis affectus. Nam quibus rebus efficiuntur voluptates, eae non sunt in potestate sapientis. Quis est tam dissimile homini. Ut proverbia non nulla veriora sint quam vestra dogmata. Si quicquam extra virtutem habeatur in bonis. Sed plane dicit quod intellegit. Paulum, cum regem Persem captum adduceret, eodem flumine invectio?',\n",
       " '',\n",
       " 'Qui ita affectus, beatum esse numquam probabis; Sed nimis multa. Nam prius a se poterit quisque discedere quam appetitum earum rerum, quae sibi conducant, amittere. Familiares nostros, credo, Sironem dicis et Philodemum, cum optimos viros, tum homines doctissimos. Quod iam a me expectare noli. Quid ergo?',\n",
       " '',\n",
       " 'Eademne, quae restincta siti? Ita relinquet duas, de quibus etiam atque etiam consideret. Illa videamus, quae a te de amicitia dicta sunt. Eaedem res maneant alio modo. Quid ergo attinet gloriose loqui, nisi constanter loquare? Prioris generis est docilitas, memoria; Portenta haec esse dicit, neque ea ratione ullo modo posse vivi; Beatum, inquit. Bestiarum vero nullum iudicium puto.',\n",
       " '',\n",
       " 'Quem Tiberina descensio festo illo die tanto gaudio affecit, quanto L. Quorum sine causa fieri nihil putandum est. Tria genera bonorum; Nunc dicam de voluptate, nihil scilicet novi, ea tamen, quae te ipsum probaturum esse confidam. Illud dico, ea, quae dicat, praeclare inter se cohaerere. Fortemne possumus dicere eundem illum Torquatum? Hoc tu nunc in illo probas. Cur post Tarentum ad Archytam?',\n",
       " '',\n",
       " 'Indicant pueri, in quibus ut in speculis natura cernitur.',\n",
       " 'Sed tamen est aliquid, quod nobis non liceat, liceat illis. Virtutis, magnitudinis animi, patientiae, fortitudinis fomentis dolor mitigari solet. Piso igitur hoc modo, vir optimus tuique, ut scis, amantissimus. Non prorsus, inquit, omnisque, qui sine dolore sint, in voluptate, et ea quidem summa, esse dico. Potius inflammat, ut coercendi magis quam dedocendi esse videantur. Virtutis, magnitudinis animi, patientiae, fortitudinis fomentis dolor mitigari solet. Quae fere omnia appellantur uno ingenii nomine, easque virtutes qui habent, ingeniosi vocantur. Nec enim, dum metuit, iustus est, et certe, si metuere destiterit, non erit;',\n",
       " 'Quod equidem non reprehendo;',\n",
       " 'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quibus natura iure responderit non esse verum aliunde finem beate vivendi, a se principia rei gerendae peti; Quae enim adhuc protulisti, popularia sunt, ego autem a te elegantiora desidero. Duo Reges: constructio interrete. Tum Lucius: Mihi vero ista valde probata sunt, quod item fratri puto. Bestiarum vero nullum iudicium puto. Nihil enim iam habes, quod ad corpus referas; Deinde prima illa, quae in congressu solemus: Quid tu, inquit, huc? Et homini, qui ceteris animantibus plurimum praestat, praecipue a natura nihil datum esse dicemus?',\n",
       " '',\n",
       " 'Iam id ipsum absurdum, maximum malum neglegi. Quod ea non occurrentia fingunt, vincunt Aristonem; Atqui perspicuum est hominem e corpore animoque constare, cum primae sint animi partes, secundae corporis. Fieri, inquam, Triari, nullo pacto potest, ut non dicas, quid non probes eius, a quo dissentias. Equidem e Cn. An dubium est, quin virtus ita maximam partem optineat in rebus humanis, ut reliquas obruat?',\n",
       " '',\n",
       " 'Quis istum dolorem timet?',\n",
       " 'Summus dolor plures dies manere non potest? Dicet pro me ipsa virtus nec dubitabit isti vestro beato M. Tubulum fuisse, qua illum, cuius is condemnatus est rogatione, P. Quod si ita sit, cur opera philosophiae sit danda nescio.',\n",
       " '',\n",
       " 'Ex eorum enim scriptis et institutis cum omnis doctrina liberalis, omnis historia.',\n",
       " 'Quod si ita est, sequitur id ipsum, quod te velle video, omnes semper beatos esse sapientes. Cum enim fertur quasi torrens oratio, quamvis multa cuiusque modi rapiat, nihil tamen teneas, nihil apprehendas, nusquam orationem rapidam coerceas. Ita redarguitur ipse a sese, convincunturque scripta eius probitate ipsius ac moribus. At quanta conantur! Mundum hunc omnem oppidum esse nostrum! Incendi igitur eos, qui audiunt, vides. Vide, ne magis, inquam, tuum fuerit, cum re idem tibi, quod mihi, videretur, non nova te rebus nomina inponere. Qui-vere falsone, quaerere mittimus-dicitur oculis se privasse; Si ista mala sunt, in quae potest incidere sapiens, sapientem esse non esse ad beate vivendum satis. At vero si ad vitem sensus accesserit, ut appetitum quendam habeat et per se ipsa moveatur, quid facturam putas?',\n",
       " '',\n",
       " 'Quem si tenueris, non modo meum Ciceronem, sed etiam me ipsum abducas licebit.',\n",
       " 'Stulti autem malorum memoria torquentur, sapientes bona praeterita grata recordatione renovata delectant.',\n",
       " 'Esse enim quam vellet iniquus iustus poterat inpune.',\n",
       " 'Quae autem natura suae primae institutionis oblita est?',\n",
       " 'Verum tamen cum de rebus grandioribus dicas, ipsae res verba rapiunt;',\n",
       " 'Hoc est non modo cor non habere, sed ne palatum quidem.',\n",
       " 'Voluptatem cum summum bonum diceret, primum in eo ipso parum vidit, deinde hoc quoque alienum; Sed tu istuc dixti bene Latine, parum plane. Nam haec ipsa mihi erunt in promptu, quae modo audivi, nec ante aggrediar, quam te ab istis, quos dicis, instructum videro. Fatebuntur Stoici haec omnia dicta esse praeclare, neque eam causam Zenoni desciscendi fuisse. Non autem hoc: igitur ne illud quidem. Ratio quidem vestra sic cogit. Cum audissem Antiochum, Brute, ut solebam, cum M. An quod ita callida est, ut optime possit architectari voluptates?',\n",
       " '',\n",
       " 'Idemne, quod iucunde?',\n",
       " 'Haec mihi videtur delicatior, ut ita dicam, molliorque ratio, quam virtutis vis gravitasque postulat. Sed quoniam et advesperascit et mihi ad villam revertendum est, nunc quidem hactenus; Cuius ad naturam apta ratio vera illa et summa lex a philosophis dicitur. Neque solum ea communia, verum etiam paria esse dixerunt. Sed nunc, quod agimus; A mene tu?']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all files and flatmap in 1 list\n",
    "textFile=sc.textFile(\"data/textFiles/*.txt\")\n",
    "# textFile=sc.textFile(\"hdfs://localhost:9000/data/sample1.txt\")\n",
    "# textFile=sc.textFile(\"data/textFiles\")\n",
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5688876c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Utilitatis causa amicitia est quaesita.',\n",
       "  'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Collatio igitur ista te nihil iuvat. Honesta oratio, Socratica, Platonis etiam. Primum in nostrane potestate est, quid meminerimus? Duo Reges: constructio interrete. Quid, si etiam iucunda memoria est praeteritorum malorum? Si quidem, inquit, tollerem, sed relinquo. An nisi populari fama?',\n",
       "  '',\n",
       "  'Quamquam id quidem licebit iis existimare, qui legerint. Summum a vobis bonum voluptas dicitur. At hoc in eo M. Refert tamen, quo modo. Quid sequatur, quid repugnet, vident. Iam id ipsum absurdum, maximum malum neglegi.'],\n",
       " ['Aeque enim contingit omnibus fidibus, ut incontentae sint.',\n",
       "  'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quae cum ita sint, effectum est nihil esse malum, quod turpe non sit. Itaque nostrum est-quod nostrum dico, artis est-ad ea principia, quae accepimus. Quod totum contra est. Duo Reges: constructio interrete. Atqui iste locus est, Piso, tibi etiam atque etiam confirmandus, inquam; Quamvis enim depravatae non sint, pravae tamen esse possunt. Duarum enim vitarum nobis erunt instituta capienda.',\n",
       "  '',\n",
       "  'Non igitur de improbo, sed de callido improbo quaerimus, qualis Q. Audio equidem philosophi vocem, Epicure, sed quid tibi dicendum sit oblitus es. Ex ea difficultate illae fallaciloquae, ut ait Accius, malitiae natae sunt. At multis malis affectus. Nam quibus rebus efficiuntur voluptates, eae non sunt in potestate sapientis. Quis est tam dissimile homini. Ut proverbia non nulla veriora sint quam vestra dogmata. Si quicquam extra virtutem habeatur in bonis. Sed plane dicit quod intellegit. Paulum, cum regem Persem captum adduceret, eodem flumine invectio?',\n",
       "  '',\n",
       "  'Qui ita affectus, beatum esse numquam probabis; Sed nimis multa. Nam prius a se poterit quisque discedere quam appetitum earum rerum, quae sibi conducant, amittere. Familiares nostros, credo, Sironem dicis et Philodemum, cum optimos viros, tum homines doctissimos. Quod iam a me expectare noli. Quid ergo?',\n",
       "  '',\n",
       "  'Eademne, quae restincta siti? Ita relinquet duas, de quibus etiam atque etiam consideret. Illa videamus, quae a te de amicitia dicta sunt. Eaedem res maneant alio modo. Quid ergo attinet gloriose loqui, nisi constanter loquare? Prioris generis est docilitas, memoria; Portenta haec esse dicit, neque ea ratione ullo modo posse vivi; Beatum, inquit. Bestiarum vero nullum iudicium puto.',\n",
       "  '',\n",
       "  'Quem Tiberina descensio festo illo die tanto gaudio affecit, quanto L. Quorum sine causa fieri nihil putandum est. Tria genera bonorum; Nunc dicam de voluptate, nihil scilicet novi, ea tamen, quae te ipsum probaturum esse confidam. Illud dico, ea, quae dicat, praeclare inter se cohaerere. Fortemne possumus dicere eundem illum Torquatum? Hoc tu nunc in illo probas. Cur post Tarentum ad Archytam?',\n",
       "  '',\n",
       "  'Indicant pueri, in quibus ut in speculis natura cernitur.',\n",
       "  'Sed tamen est aliquid, quod nobis non liceat, liceat illis. Virtutis, magnitudinis animi, patientiae, fortitudinis fomentis dolor mitigari solet. Piso igitur hoc modo, vir optimus tuique, ut scis, amantissimus. Non prorsus, inquit, omnisque, qui sine dolore sint, in voluptate, et ea quidem summa, esse dico. Potius inflammat, ut coercendi magis quam dedocendi esse videantur. Virtutis, magnitudinis animi, patientiae, fortitudinis fomentis dolor mitigari solet. Quae fere omnia appellantur uno ingenii nomine, easque virtutes qui habent, ingeniosi vocantur. Nec enim, dum metuit, iustus est, et certe, si metuere destiterit, non erit;'],\n",
       " ['Quod equidem non reprehendo;',\n",
       "  'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quibus natura iure responderit non esse verum aliunde finem beate vivendi, a se principia rei gerendae peti; Quae enim adhuc protulisti, popularia sunt, ego autem a te elegantiora desidero. Duo Reges: constructio interrete. Tum Lucius: Mihi vero ista valde probata sunt, quod item fratri puto. Bestiarum vero nullum iudicium puto. Nihil enim iam habes, quod ad corpus referas; Deinde prima illa, quae in congressu solemus: Quid tu, inquit, huc? Et homini, qui ceteris animantibus plurimum praestat, praecipue a natura nihil datum esse dicemus?',\n",
       "  '',\n",
       "  'Iam id ipsum absurdum, maximum malum neglegi. Quod ea non occurrentia fingunt, vincunt Aristonem; Atqui perspicuum est hominem e corpore animoque constare, cum primae sint animi partes, secundae corporis. Fieri, inquam, Triari, nullo pacto potest, ut non dicas, quid non probes eius, a quo dissentias. Equidem e Cn. An dubium est, quin virtus ita maximam partem optineat in rebus humanis, ut reliquas obruat?',\n",
       "  '',\n",
       "  'Quis istum dolorem timet?',\n",
       "  'Summus dolor plures dies manere non potest? Dicet pro me ipsa virtus nec dubitabit isti vestro beato M. Tubulum fuisse, qua illum, cuius is condemnatus est rogatione, P. Quod si ita sit, cur opera philosophiae sit danda nescio.',\n",
       "  '',\n",
       "  'Ex eorum enim scriptis et institutis cum omnis doctrina liberalis, omnis historia.',\n",
       "  'Quod si ita est, sequitur id ipsum, quod te velle video, omnes semper beatos esse sapientes. Cum enim fertur quasi torrens oratio, quamvis multa cuiusque modi rapiat, nihil tamen teneas, nihil apprehendas, nusquam orationem rapidam coerceas. Ita redarguitur ipse a sese, convincunturque scripta eius probitate ipsius ac moribus. At quanta conantur! Mundum hunc omnem oppidum esse nostrum! Incendi igitur eos, qui audiunt, vides. Vide, ne magis, inquam, tuum fuerit, cum re idem tibi, quod mihi, videretur, non nova te rebus nomina inponere. Qui-vere falsone, quaerere mittimus-dicitur oculis se privasse; Si ista mala sunt, in quae potest incidere sapiens, sapientem esse non esse ad beate vivendum satis. At vero si ad vitem sensus accesserit, ut appetitum quendam habeat et per se ipsa moveatur, quid facturam putas?',\n",
       "  '',\n",
       "  'Quem si tenueris, non modo meum Ciceronem, sed etiam me ipsum abducas licebit.',\n",
       "  'Stulti autem malorum memoria torquentur, sapientes bona praeterita grata recordatione renovata delectant.',\n",
       "  'Esse enim quam vellet iniquus iustus poterat inpune.',\n",
       "  'Quae autem natura suae primae institutionis oblita est?',\n",
       "  'Verum tamen cum de rebus grandioribus dicas, ipsae res verba rapiunt;',\n",
       "  'Hoc est non modo cor non habere, sed ne palatum quidem.',\n",
       "  'Voluptatem cum summum bonum diceret, primum in eo ipso parum vidit, deinde hoc quoque alienum; Sed tu istuc dixti bene Latine, parum plane. Nam haec ipsa mihi erunt in promptu, quae modo audivi, nec ante aggrediar, quam te ab istis, quos dicis, instructum videro. Fatebuntur Stoici haec omnia dicta esse praeclare, neque eam causam Zenoni desciscendi fuisse. Non autem hoc: igitur ne illud quidem. Ratio quidem vestra sic cogit. Cum audissem Antiochum, Brute, ut solebam, cum M. An quod ita callida est, ut optime possit architectari voluptates?',\n",
       "  '',\n",
       "  'Idemne, quod iucunde?',\n",
       "  'Haec mihi videtur delicatior, ut ita dicam, molliorque ratio, quam virtutis vis gravitasque postulat. Sed quoniam et advesperascit et mihi ad villam revertendum est, nunc quidem hactenus; Cuius ad naturam apta ratio vera illa et summa lex a philosophis dicitur. Neque solum ea communia, verum etiam paria esse dixerunt. Sed nunc, quod agimus; A mene tu?']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94769bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b2cf753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Utilitatis causa amicitia est quaesita.',\n",
       " 'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Collatio igitur ista te nihil iuvat. Honesta oratio, Socratica, Platonis etiam. Primum in nostrane potestate est, quid meminerimus? Duo Reges: constructio interrete. Quid, si etiam iucunda memoria est praeteritorum malorum? Si quidem, inquit, tollerem, sed relinquo. An nisi populari fama?',\n",
       " '',\n",
       " 'Quamquam id quidem licebit iis existimare, qui legerint. Summum a vobis bonum voluptas dicitur. At hoc in eo M. Refert tamen, quo modo. Quid sequatur, quid repugnet, vident. Iam id ipsum absurdum, maximum malum neglegi.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read from 1 file\n",
    "textFile=sc.textFile(\"data/textFiles/sample1.txt\")\n",
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24a9981f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/D:/Projects/Python/pyspark/Topic-wise/data/textFiles/sample1.txt',\n",
       "  'Utilitatis causa amicitia est quaesita.\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Collatio igitur ista te nihil iuvat. Honesta oratio, Socratica, Platonis etiam. Primum in nostrane potestate est, quid meminerimus? Duo Reges: constructio interrete. Quid, si etiam iucunda memoria est praeteritorum malorum? Si quidem, inquit, tollerem, sed relinquo. An nisi populari fama?\\n\\nQuamquam id quidem licebit iis existimare, qui legerint. Summum a vobis bonum voluptas dicitur. At hoc in eo M. Refert tamen, quo modo. Quid sequatur, quid repugnet, vident. Iam id ipsum absurdum, maximum malum neglegi.'),\n",
       " ('file:/D:/Projects/Python/pyspark/Topic-wise/data/textFiles/sample2.txt',\n",
       "  'Aeque enim contingit omnibus fidibus, ut incontentae sint.\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Quae cum ita sint, effectum est nihil esse malum, quod turpe non sit. Itaque nostrum est-quod nostrum dico, artis est-ad ea principia, quae accepimus. Quod totum contra est. Duo Reges: constructio interrete. Atqui iste locus est, Piso, tibi etiam atque etiam confirmandus, inquam; Quamvis enim depravatae non sint, pravae tamen esse possunt. Duarum enim vitarum nobis erunt instituta capienda.\\n\\nNon igitur de improbo, sed de callido improbo quaerimus, qualis Q. Audio equidem philosophi vocem, Epicure, sed quid tibi dicendum sit oblitus es. Ex ea difficultate illae fallaciloquae, ut ait Accius, malitiae natae sunt. At multis malis affectus. Nam quibus rebus efficiuntur voluptates, eae non sunt in potestate sapientis. Quis est tam dissimile homini. Ut proverbia non nulla veriora sint quam vestra dogmata. Si quicquam extra virtutem habeatur in bonis. Sed plane dicit quod intellegit. Paulum, cum regem Persem captum adduceret, eodem flumine invectio?\\n\\nQui ita affectus, beatum esse numquam probabis; Sed nimis multa. Nam prius a se poterit quisque discedere quam appetitum earum rerum, quae sibi conducant, amittere. Familiares nostros, credo, Sironem dicis et Philodemum, cum optimos viros, tum homines doctissimos. Quod iam a me expectare noli. Quid ergo?\\n\\nEademne, quae restincta siti? Ita relinquet duas, de quibus etiam atque etiam consideret. Illa videamus, quae a te de amicitia dicta sunt. Eaedem res maneant alio modo. Quid ergo attinet gloriose loqui, nisi constanter loquare? Prioris generis est docilitas, memoria; Portenta haec esse dicit, neque ea ratione ullo modo posse vivi; Beatum, inquit. Bestiarum vero nullum iudicium puto.\\n\\nQuem Tiberina descensio festo illo die tanto gaudio affecit, quanto L. Quorum sine causa fieri nihil putandum est. Tria genera bonorum; Nunc dicam de voluptate, nihil scilicet novi, ea tamen, quae te ipsum probaturum esse confidam. Illud dico, ea, quae dicat, praeclare inter se cohaerere. Fortemne possumus dicere eundem illum Torquatum? Hoc tu nunc in illo probas. Cur post Tarentum ad Archytam?\\n\\nIndicant pueri, in quibus ut in speculis natura cernitur.\\nSed tamen est aliquid, quod nobis non liceat, liceat illis. Virtutis, magnitudinis animi, patientiae, fortitudinis fomentis dolor mitigari solet. Piso igitur hoc modo, vir optimus tuique, ut scis, amantissimus. Non prorsus, inquit, omnisque, qui sine dolore sint, in voluptate, et ea quidem summa, esse dico. Potius inflammat, ut coercendi magis quam dedocendi esse videantur. Virtutis, magnitudinis animi, patientiae, fortitudinis fomentis dolor mitigari solet. Quae fere omnia appellantur uno ingenii nomine, easque virtutes qui habent, ingeniosi vocantur. Nec enim, dum metuit, iustus est, et certe, si metuere destiterit, non erit;')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all files as tuple in key as path and value as text file with '\\n'\n",
    "textFile=sc.wholeTextFiles(\"data/textFiles\")\n",
    "textFile.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f14b1",
   "metadata": {},
   "source": [
    "# Retrieving RDD info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bb3f27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions() # equals total number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9874c3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()  # Count RDD instances\n",
    "#[('a', 7), ('a', 2), ('b', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25071514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 2, 'b': 1})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByKey() # only key counted\n",
    "#[('a', 7), ('a', 2), ('b', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80bcf20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {('a', 7): 1, ('a', 2): 1, ('b', 2): 1})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByValue() # the whole () is considered and counted\n",
    "#[('a', 7), ('a', 2), ('b', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64330ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [('a', 7)], [('a', 2)], [('b', 2)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': 2, 'b': 2}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rdd.glom().collect())\n",
    "rdd.collectAsMap() #Return (key,value) pairs as a dictionary\n",
    "#[('a', 7), ('a', 2), ('b', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce4b1f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.sum()\n",
    "# range(100) -> 0-99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "912e79f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether RDD is emp\n",
    "sc.parallelize([]).isEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca4b207c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# range(100) -> 0-99\n",
    "rdd3.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "613f56f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc72dba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.mean() #mean is the average of a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f58824fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.86607004772212"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''A standard deviation is a statistic that measures the dispersion of a \n",
    "dataset relative to its mean. The standard deviation is calculated as the \n",
    "square root of variance by determining each data point's deviation relative \n",
    "to the mean. If the data points are further from the mean, there is a \n",
    "higher deviation within the data set; thus, the more spread out the data, \n",
    "the higher the standard deviation.'''\n",
    "rdd3.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86b9386f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "833.25"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''variance is the expectation of the squared deviation of a random variable \n",
    "from its mean. Variance is a measure of dispersion, meaning it is a measure \n",
    "of how far a set of numbers is spread out from their average value.'''\n",
    "rdd3.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56ce24e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 33, 66, 99], [33, 33, 34])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.histogram(3) #Compute histogram by bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cdcbcf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 100, mean: 49.5, stdev: 28.86607004772212, max: 99.0, min: 0.0)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f9b6f0",
   "metadata": {},
   "source": [
    "# Selecting data (Getting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f86f11ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2), ('b', 2)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect() #Return a list with all RDD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "21be62f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2), ('b', 2)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5) #Take first 5 RDD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f8f9d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 2), ('a', 7)]\n",
      "[('a', 7), ('a', 2), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.takeOrdered(5,key = lambda x: x[1]))\n",
    "print(rdd.takeOrdered(5,key = lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9c2543ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DMS240~1\\AppData\\Local\\Temp/ipykernel_30624/808162870.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtakeSample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtakeSample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtakeSample\u001b[1;34m(self, withReplacement, num, seed)\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[0mrand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "print(rdd3.takeSample(False,0.5))\n",
    "print(rdd3.takeSample(True,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc5ed703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 7)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a246da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('a', 7), ('a', 2)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.top(3) #Take top 3 RDD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b6ed62c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(4) PythonRDD[54] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []'\n"
     ]
    }
   ],
   "source": [
    "print(rdd.filter(lambda x: x[1] > 2).toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a7b13f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74], [75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "[300, 925, 1550, 2175]\n"
     ]
    }
   ],
   "source": [
    "print(rdd3.glom().collect())\n",
    "def f1(lists): \n",
    "    return lists\n",
    "def f2(lists): \n",
    "    yield sum(lists)\n",
    "print(rdd3.mapPartitions(f1).collect())\n",
    "print(rdd3.mapPartitions(f2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7747a34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, range(0, 25)), (1, range(25, 50)), (2, range(50, 75)), (3, range(75, 100))]\n",
      "[0, range(0, 25), 1, range(25, 50), 2, range(50, 75), 3, range(75, 100)]\n"
     ]
    }
   ],
   "source": [
    "def f1(splitIndex, iterator): yield (splitIndex,iterator)\n",
    "def f2(splitIndex, iterator): \n",
    "    return (splitIndex,iterator)\n",
    "print(rdd3.mapPartitionsWithIndex(f1).collect())\n",
    "print(rdd3.mapPartitionsWithIndex(f2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e50bbb",
   "metadata": {},
   "source": [
    "# Selecting data (Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8d6ba47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 26, 39, 41, 42, 52, 63, 76, 80, 86, 97]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.sample(False,0.10,81).collect()\n",
    "# with replacement=false, total=10%, seed=81"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f34b5",
   "metadata": {},
   "source": [
    "# Selecting data (Filterig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c2d3b5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 7), ('a', 2)]\n",
      "[('a', 7)]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.filter(lambda x: \"a\" in x).collect())\n",
    "print(rdd.filter(lambda x: 7 in x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "42fdd883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1), ('d', 1)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.distinct().collect()  # Return distinct RDD values\n",
    "# [('a', 2), ('d', 1), ('b', 1), ('b', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d7b9594c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'b']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.keys().collect() # Return (key,value) RDD's keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a619a0b",
   "metadata": {},
   "source": [
    "# Iterating (prints on console and not here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5dcc0ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.stdout = open(1, 'w')\n",
    "def f(x): print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
    "print('this will show up in the terminal', file=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d344188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.stdout = open(1, 'w')\n",
    "def f(x): print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
    "print('this will show up in the terminal', file=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea31bf4",
   "metadata": {},
   "source": [
    "# Applying functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca62293b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7, 7, 'a'), ('a', 2, 2, 'a'), ('b', 2, 2, 'b')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a function to each RDD element \n",
    "rdd.map(lambda x: x+(x[1],x[0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b1869ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 7, 7, 'a', 'a', 2, 2, 'a', 'b', 2, 2, 'b']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a function to each RDD element and flatten the result\n",
    "rdd.flatMap(lambda x: x+(x[1],x[0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dea79e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a flatMap function to each (key,value) pair of rdd4 without changing the keys\n",
    "rdd4.flatMapValues(lambda x: x).collect() \n",
    "# [('a', ['x', 'y', 'z']), ('b', ['p', 'r'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07871eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', ['A', 'B', 'C']),\n",
       " ('a', ['x', 'y', 'z']),\n",
       " ('b', ['A', 'B', 'C']),\n",
       " ('b', ['p', 'r'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a flatMap function to each (key,value) pair of rdd4 without changing the keys\n",
    "rdd4.flatMapValues(lambda x: (list('ABC'), x)).collect() \n",
    "# [('a', ['x', 'y', 'z']), ('b', ['p', 'r'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b84373c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', ('A', 'x')),\n",
       " ('a', ('B', 'y')),\n",
       " ('a', ('C', 'z')),\n",
       " ('b', ('A', 'p')),\n",
       " ('b', ('B', 'r'))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.flatMapValues(lambda x: list(zip(list('ABC'), x))).collect() \n",
    "# [('a', ['x', 'y', 'z']), ('b', ['p', 'r'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67f219a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('a', 9)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupByKey().mapValues(lambda x: sum(x)).collect()\n",
    "# [('a', 7), ('a', 2), ('b', 2)] -> [('b', [2]), ('a', [7, 2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31774020",
   "metadata": {},
   "source": [
    "# Sort and Order By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b1889edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', 1), ('b', 1), ('b', 1), ('a', 2)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.sortBy(lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e121106e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1), ('b', 1), ('d', 1)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6c054b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orderBy in DF only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bce8c1",
   "metadata": {},
   "source": [
    "# Mathematical Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0349819d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2), ('b', 2), ('a', 2), ('d', 1), ('b', 1), ('b', 1)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rdd+rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b5ac48ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('a', 7)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.subtract(rdd2).collect()\n",
    "# [('a', 7), ('a', 2), ('b', 2)]\n",
    "# [('a', 2), ('d', 1), ('b', 1), ('b', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ce203d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', 1)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.subtractByKey(rdd).collect()\n",
    "# [('a', 2), ('d', 1), ('b', 1), ('b', 1)]\n",
    "# [('a', 7), ('a', 2), ('b', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d12ae559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', 7), ('a', 2)),\n",
       " (('a', 7), ('d', 1)),\n",
       " (('a', 7), ('b', 1)),\n",
       " (('a', 7), ('b', 1)),\n",
       " (('a', 2), ('a', 2)),\n",
       " (('a', 2), ('d', 1)),\n",
       " (('a', 2), ('b', 1)),\n",
       " (('a', 2), ('b', 1)),\n",
       " (('b', 2), ('a', 2)),\n",
       " (('b', 2), ('d', 1)),\n",
       " (('b', 2), ('b', 1)),\n",
       " (('b', 2), ('b', 1))]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.cartesian(rdd2).collect() #Return the Cartesian product of rdd and rdd2\n",
    "# [('a', 7), ('a', 2), ('b', 2)]\n",
    "# [('a', 2), ('d', 1), ('b', 1), ('b', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "70b49f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 7), ('a', 2), ('b', 2), ('a', 2), ('d', 1), ('b', 1), ('b', 1)]\n",
      "[('a', 2), ('d', 1), ('b', 1), ('b', 1), ('a', 7), ('a', 2), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.union(rdd2).collect())\n",
    "print(rdd2.union(rdd).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f906db51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2)]\n",
      "[('a', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.intersection(rdd2).collect())\n",
    "print(rdd2.intersection(rdd).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff9774",
   "metadata": {},
   "source": [
    "# Reshaping data (Reducing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "77f96fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('a', 9)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda x,y: x+y).collect()\n",
    "# [('a', 7), ('a', 2), ('b', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ba0d71fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 7, 'a', 2, 'b', 2)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce(lambda x,y: x+y)\n",
    "# [('a', 7), ('a', 2), ('b', 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8196898f",
   "metadata": {},
   "source": [
    "# Reshaping data (Grouping By)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3ef55a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [0,\n",
       "   2,\n",
       "   4,\n",
       "   6,\n",
       "   8,\n",
       "   10,\n",
       "   12,\n",
       "   14,\n",
       "   16,\n",
       "   18,\n",
       "   20,\n",
       "   22,\n",
       "   24,\n",
       "   26,\n",
       "   28,\n",
       "   30,\n",
       "   32,\n",
       "   34,\n",
       "   36,\n",
       "   38,\n",
       "   40,\n",
       "   42,\n",
       "   44,\n",
       "   46,\n",
       "   48,\n",
       "   50,\n",
       "   52,\n",
       "   54,\n",
       "   56,\n",
       "   58,\n",
       "   60,\n",
       "   62,\n",
       "   64,\n",
       "   66,\n",
       "   68,\n",
       "   70,\n",
       "   72,\n",
       "   74,\n",
       "   76,\n",
       "   78,\n",
       "   80,\n",
       "   82,\n",
       "   84,\n",
       "   86,\n",
       "   88,\n",
       "   90,\n",
       "   92,\n",
       "   94,\n",
       "   96,\n",
       "   98])]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.groupBy(lambda x: x%2).mapValues(list).collect()\n",
    "# range(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c18ade3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', [2]), ('a', [7, 2])]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupByKey().mapValues(list).collect()\n",
    "# [('a', 7), ('a', 2), ('b', 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6009a72",
   "metadata": {},
   "source": [
    "# Reshaping data (Zip with Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ec44887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', 7), 0), (('a', 2), 1), (('b', 2), 2)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.zipWithIndex().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0ca5d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 'a', 7], [1, 'a', 2], [2, 'b', 2]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.zipWithIndex().map(lambda x: [x[1]] + list(x[0])).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ddeef2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "|index|char|count|\n",
      "+-----+----+-----+\n",
      "|    0|   a|    7|\n",
      "|    1|   a|    2|\n",
      "|    2|   b|    2|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "header = ['index'] + ['char','count']\n",
    "rdd.zipWithIndex().map(lambda x: [x[1]] + list(x[0])).toDF(header).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a518f52",
   "metadata": {},
   "source": [
    "# Reshaping data (Aggregating)\n",
    "aggregate(zeroValue, seqOp, combOp)\n",
    "\n",
    "aggregateByKey(zeroValue, seqOp, combOp)\n",
    "- zeroValue: is like a data container. Its structure should match with the data structure of the returned values from the seqOp function.\n",
    "- seqOp: is a function that takes two arguments: the first argument is the zeroValue and the second argument is an element from the RDD. The zeroValue gets updated with the returned value after every run.\n",
    "- combOp: is a function that takes two arguments: the first argument is the final zeroValue from one partition and the other is another final zeroValue from another partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6caf5ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4950, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqOp = (lambda x,y: (x[0]+y, x[1]+1))\n",
    "combOp = (lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "#Aggregate RDD elements of each partition and then the results\n",
    "rdd3.aggregate(\n",
    "    (0,0),\n",
    "    seqOp, # combine values within partition\n",
    "    combOp) # combine values across partition\n",
    "# range(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "374cff5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqOp = (lambda x,y: (x+y))\n",
    "combOp = (lambda x,y: (x+y))\n",
    "#Aggregate RDD elements of each partition and then the results\n",
    "rdd3.aggregate(\n",
    "    0,\n",
    "    seqOp, # combine values within partition\n",
    "    combOp) # combine values across partition\n",
    "# range(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6d3ccbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (2, 1)), ('a', (9, 2))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aggregate values of each RDD key\n",
    "seqOp = (lambda x,y: (x[0]+y, x[1]+1))\n",
    "combOp = (lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "rdd.aggregateByKey(\n",
    "    (0,0),\n",
    "    seqOp, # combine values within partition\n",
    "    combOp # combine values across partition\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8493bc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('a', 9)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqOp = (lambda x,y: (x+y))\n",
    "combOp = (lambda x,y: (x+y))\n",
    "rdd.aggregateByKey(\n",
    "    0,\n",
    "    seqOp, # combine values within partition\n",
    "    combOp # combine values across partition\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "65871ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aggregate the elements of each partition, and then the results\n",
    "def add(a,b): return a+b\n",
    "rdd3.fold(0,add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c1629e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('a', 9)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge the values for each key\n",
    "rdd.foldByKey(0, add).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2c3c3c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (2, 1),\n",
       " (4, 2),\n",
       " (6, 3),\n",
       " (8, 4),\n",
       " (10, 5),\n",
       " (12, 6),\n",
       " (14, 7),\n",
       " (16, 8),\n",
       " (18, 9),\n",
       " (20, 10),\n",
       " (22, 11),\n",
       " (24, 12),\n",
       " (26, 13),\n",
       " (28, 14),\n",
       " (30, 15),\n",
       " (32, 16),\n",
       " (34, 17),\n",
       " (36, 18),\n",
       " (38, 19),\n",
       " (40, 20),\n",
       " (42, 21),\n",
       " (44, 22),\n",
       " (46, 23),\n",
       " (48, 24),\n",
       " (50, 25),\n",
       " (52, 26),\n",
       " (54, 27),\n",
       " (56, 28),\n",
       " (58, 29),\n",
       " (60, 30),\n",
       " (62, 31),\n",
       " (64, 32),\n",
       " (66, 33),\n",
       " (68, 34),\n",
       " (70, 35),\n",
       " (72, 36),\n",
       " (74, 37),\n",
       " (76, 38),\n",
       " (78, 39),\n",
       " (80, 40),\n",
       " (82, 41),\n",
       " (84, 42),\n",
       " (86, 43),\n",
       " (88, 44),\n",
       " (90, 45),\n",
       " (92, 46),\n",
       " (94, 47),\n",
       " (96, 48),\n",
       " (98, 49),\n",
       " (100, 50),\n",
       " (102, 51),\n",
       " (104, 52),\n",
       " (106, 53),\n",
       " (108, 54),\n",
       " (110, 55),\n",
       " (112, 56),\n",
       " (114, 57),\n",
       " (116, 58),\n",
       " (118, 59),\n",
       " (120, 60),\n",
       " (122, 61),\n",
       " (124, 62),\n",
       " (126, 63),\n",
       " (128, 64),\n",
       " (130, 65),\n",
       " (132, 66),\n",
       " (134, 67),\n",
       " (136, 68),\n",
       " (138, 69),\n",
       " (140, 70),\n",
       " (142, 71),\n",
       " (144, 72),\n",
       " (146, 73),\n",
       " (148, 74),\n",
       " (150, 75),\n",
       " (152, 76),\n",
       " (154, 77),\n",
       " (156, 78),\n",
       " (158, 79),\n",
       " (160, 80),\n",
       " (162, 81),\n",
       " (164, 82),\n",
       " (166, 83),\n",
       " (168, 84),\n",
       " (170, 85),\n",
       " (172, 86),\n",
       " (174, 87),\n",
       " (176, 88),\n",
       " (178, 89),\n",
       " (180, 90),\n",
       " (182, 91),\n",
       " (184, 92),\n",
       " (186, 93),\n",
       " (188, 94),\n",
       " (190, 95),\n",
       " (192, 96),\n",
       " (194, 97),\n",
       " (196, 98),\n",
       " (198, 99)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create tuples of RDD elements by applying a function\n",
    "rdd3.keyBy(lambda x: x+x).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b1281",
   "metadata": {},
   "source": [
    "# Repartitioning / coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f581a008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [Row(val='a', count=7)],\n",
       " [],\n",
       " [Row(val='a', count=2), Row(val='b', count=2)],\n",
       " []]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "rdd.toDF([\"val\",\"count\"]).repartition(5,col(\"count\")).rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "26bb9175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(val='a', count=7), Row(val='a', count=2)],\n",
       " [Row(val='b', count=2)],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.toDF([\"val\",\"count\"]).repartition(5,col(\"val\")).rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d01e499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2), ('b', 2)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.coalesce(1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9e808",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ab123d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsTextFile('save/rdddemo1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5cf4f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.coalesce(1).saveAsTextFile('save/rdddemo2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "854cda0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.coalesce(1).saveAsPickleFile('save/rdddemo3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f9dc9f16",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsHadoopFile.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.api.python.PythonRDD$.saveAsHadoopFile(PythonRDD.scala:555)\r\n\tat org.apache.spark.api.python.PythonRDD.saveAsHadoopFile(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 100.0 failed 1 times, most recent failure: Lost task 0.0 in stage 100.0 (TID 408, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /save/rdddemo6/_temporary/0/_temporary/attempt_20210912164124_0175_m_000000_0/part-00000 could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1620)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3135)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3059)\r\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:725)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:493)\r\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2211)\r\n\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\r\n\tat com.sun.proxy.$Proxy26.addBlock(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\n\tat com.sun.proxy.$Proxy27.addBlock(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1455)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1251)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 27 more\r\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /save/rdddemo6/_temporary/0/_temporary/attempt_20210912164124_0175_m_000000_0/part-00000 could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1620)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3135)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3059)\r\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:725)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:493)\r\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2211)\r\n\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\r\n\tat com.sun.proxy.$Proxy26.addBlock(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\n\tat com.sun.proxy.$Proxy27.addBlock(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1455)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1251)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DMS240~1\\AppData\\Local\\Temp/ipykernel_30624/2197125234.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m rdd.coalesce(1).saveAsHadoopFile(\"hdfs://localhost:9000/save/rdddemo6\",\n\u001b[1;32m----> 2\u001b[1;33m                                   \"org.apache.hadoop.mapred.TextOutputFormat\")\n\u001b[0m",
      "\u001b[1;32mC:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msaveAsHadoopFile\u001b[1;34m(self, path, outputFormatClass, keyClass, valueClass, keyConverter, valueConverter, conf, compressionCodecClass)\u001b[0m\n\u001b[0;32m   1483\u001b[0m                                                  \u001b[0mkeyClass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalueClass\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m                                                  \u001b[0mkeyConverter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalueConverter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m                                                  jconf, compressionCodecClass)\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msaveAsSequenceFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompressionCodecClass\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\work\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsHadoopFile.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.api.python.PythonRDD$.saveAsHadoopFile(PythonRDD.scala:555)\r\n\tat org.apache.spark.api.python.PythonRDD.saveAsHadoopFile(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 100.0 failed 1 times, most recent failure: Lost task 0.0 in stage 100.0 (TID 408, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /save/rdddemo6/_temporary/0/_temporary/attempt_20210912164124_0175_m_000000_0/part-00000 could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1620)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3135)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3059)\r\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:725)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:493)\r\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2211)\r\n\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\r\n\tat com.sun.proxy.$Proxy26.addBlock(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\n\tat com.sun.proxy.$Proxy27.addBlock(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1455)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1251)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 27 more\r\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /save/rdddemo6/_temporary/0/_temporary/attempt_20210912164124_0175_m_000000_0/part-00000 could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1620)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3135)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3059)\r\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:725)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:493)\r\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2211)\r\n\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\r\n\tat com.sun.proxy.$Proxy26.addBlock(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\n\tat com.sun.proxy.$Proxy27.addBlock(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1455)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1251)\r\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448)\r\n"
     ]
    }
   ],
   "source": [
    "rdd.coalesce(1).saveAsHadoopFile(\"hdfs://localhost:9000/save/rdddemo6\",\n",
    "                                  \"org.apache.hadoop.mapred.TextOutputFormat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c92ee5b",
   "metadata": {},
   "source": [
    "# Persist / Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bdd5362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd5=rdd.map(lambda x: (x[1],x[0]))\n",
    "print(rdd.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998e6052",
   "metadata": {},
   "source": [
    "## Persist\n",
    "persist(Sortage Level): Persisting the RDD with the default storage level\n",
    "unpersist(): Marking the RDD as non persistent and removing the block from memory and disk\n",
    "\n",
    "1. MEMORY_ONLY (default) \n",
    "2. MEMORY_AND_DISK\n",
    "3. MEMORY_ONLY_SER\n",
    "4. MEMORY_ONLY_DISK_SER\n",
    "5. DISC_ONLY\n",
    "6. MEMORY_ONLY_2\n",
    "7. MEMORY_AND_DISK_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6febd0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[24] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []\n",
      "\n",
      "(4) PythonRDD[24] at RDD at PythonRDD.scala:53 [Disk Serialized 1x Replicated]\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 [Disk Serialized 1x Replicated]\n",
      "\n",
      "(4) PythonRDD[24] at RDD at PythonRDD.scala:53 [Disk Serialized 1x Replicated]\n",
      " |       CachedPartitions: 4; MemorySize: 0.0 B; ExternalBlockStoreSize: 0.0 B; DiskSize: 216.0 B\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 [Disk Serialized 1x Replicated]\n",
      "\n",
      "(4) PythonRDD[24] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")\n",
    "rdd5.persist(StorageLevel.DISK_ONLY)\n",
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")\n",
    "rdd5.count()\n",
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")\n",
    "rdd5.unpersist()\n",
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3cf6e857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[24] at RDD at PythonRDD.scala:53 [Disk Serialized 1x Replicated]\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 [Disk Serialized 1x Replicated]\n",
      "\n",
      "(4) PythonRDD[27] at RDD at PythonRDD.scala:53 []\n",
      " |  PythonRDD[24] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []\n",
      "\n",
      "(4) PythonRDD[24] at RDD at PythonRDD.scala:53 [Disk Serialized 1x Replicated]\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 [Disk Serialized 1x Replicated]\n",
      "\n",
      "(4) PythonRDD[24] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd5.persist(StorageLevel.DISK_ONLY)\n",
    "rdd5.storageLevel\n",
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")\n",
    "rdd6=rdd5.filter(lambda x: x[1]=='a')\n",
    "print(rdd6.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")\n",
    "rdd6.unpersist()\n",
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")\n",
    "rdd5.unpersist()\n",
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecebd13b",
   "metadata": {},
   "source": [
    "## Cache\n",
    "It is used to avoid unnecessary recomputation, same as persist(MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "137d4897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[24] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []\n",
      "\n",
      "(4) PythonRDD[24] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 [Memory Serialized 1x Replicated]\n",
      "\n",
      "(4) PythonRDD[24] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n",
      " |       CachedPartitions: 4; MemorySize: 216.0 B; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 [Memory Serialized 1x Replicated]\n",
      "\n",
      "(4) PythonRDD[24] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")\n",
    "rdd5.cache()\n",
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")\n",
    "rdd5.count()\n",
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")\n",
    "rdd5.unpersist()\n",
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2efa8a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[24] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []\n",
      "\n",
      "(4) PythonRDD[30] at RDD at PythonRDD.scala:53 []\n",
      " |  PythonRDD[24] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []\n",
      "\n",
      "(4) PythonRDD[24] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 [Memory Serialized 1x Replicated]\n",
      "\n",
      "(4) PythonRDD[24] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")\n",
    "rdd5.cache()\n",
    "rdd6=rdd5.filter(lambda x: x[1]=='a')\n",
    "print(rdd6.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")\n",
    "rdd6.unpersist()\n",
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")\n",
    "rdd5.unpersist()\n",
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d835ed",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "It saves a file inside the checkpoint directory and all the reference of its parent RDD will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b620eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[17] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []\n",
      "\n",
      "(4) PythonRDD[17] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []\n",
      "\n",
      "(4) PythonRDD[17] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")\n",
    "rdd5.checkpoint\n",
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")\n",
    "rdd5.count()\n",
    "print(rdd5.toDebugString().decode(\"utf-8\"),end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc6ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
